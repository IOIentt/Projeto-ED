{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Interconnections: Conspiracies Behind Wildfire Causes in Portugal (part 2 - Web sraping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation for visual analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Bernardo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Bernardo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk import FreqDist, ngrams\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from matplotlib_venn import venn2, venn3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Carregar o CSV e Adicionar a Coluna \"contents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['termo', 'site', 'ano', 'titulo', 'data', 'link', 'contents'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Carregar o CSV original\n",
    "df_artigos = pd.read_csv('detalhes_artigos_liquidos.csv')\n",
    "\n",
    "# Adicionar uma coluna vazia 'contents'\n",
    "df_artigos['contents'] = None\n",
    "\n",
    "# Verificar as colunas\n",
    "print(df_artigos.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Função para Remover Frases e Palavras Indesejadas [Improved] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def clean_text(text, language='portuguese'):\n",
    "    if not text:  # Handle None or empty inputs\n",
    "        return \"\"\n",
    "    \n",
    "    # Normalize Unicode (e.g., remove accents)\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \n",
    "    # List of unwanted words/phrases\n",
    "    unwanted_phrases = [\n",
    "        r\"\\bsite\\b\", r\"\\butiliza\\b\", r\"\\bnavegar\\b\", r\"\\bconsentir\\b\", r\"\\butilizacao\\b\",\n",
    "        r\"\\bsaiba\\b\", r\"\\balterar\\b\", r\"\\blocalizacao\\b\", r\"\\besqueceu\\b\", r\"\\bpalavra\\b\",\n",
    "        r\"\\bchave\\b\", r\"\\btopicos\\b\", r\"\\bsobre\\b\", r\"\\buso\\b\", r\"\\buso\\b\", r\"\\bnavegador\\b\"\n",
    "    ]\n",
    "    \n",
    "    # Remove unwanted phrases\n",
    "    for phrase in unwanted_phrases:\n",
    "        text = re.sub(phrase, '', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    # Tokenize with a regex to handle words efficiently\n",
    "    tokenizer = RegexpTokenizer(r'\\b\\w+\\b')\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words(language))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Join the tokens back into a string\n",
    "    cleaned_text = \" \".join(filtered_tokens)\n",
    "    return cleaned_text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Função de Scraping de Artigos [Improved] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "def scrape_and_clean_article(url):\n",
    "    try:\n",
    "        # Configure retries and timeout\n",
    "        session = requests.Session()\n",
    "        retries = Retry(total=3, backoff_factor=0.3, status_forcelist=[500, 502, 503, 504])\n",
    "        session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "        session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "        \n",
    "        # Fetch the webpage\n",
    "        response = session.get(url, timeout=10)\n",
    "        response.encoding = response.apparent_encoding  # Handle encoding\n",
    "        response.raise_for_status()  # Check for HTTP request errors\n",
    "        \n",
    "        # Parse the HTML\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract title\n",
    "        title_tag = soup.find('h1')\n",
    "        title_text = title_tag.get_text(strip=True) if title_tag else \"Título não encontrado\"\n",
    "        \n",
    "        # Extract content\n",
    "        content_div = soup.find('div', {'class': 'story__body'})  # Adaptable class name\n",
    "        if content_div:\n",
    "            content = \"\\n\".join([p.get_text(strip=True) for p in content_div.find_all('p')])\n",
    "        else:\n",
    "            content = \"\\n\".join([p.get_text(strip=True) for p in soup.find_all('p')])\n",
    "        \n",
    "        # Clean the extracted content\n",
    "        cleaned_content = clean_text(content)\n",
    "        return cleaned_content, title_text  # Return both content and title\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar {url}: {e}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Loop de Scraping e Preenchimento da Coluna \"contents\" (3m26seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao processar https://arquivo.pt/noFrame/replay/20170815152855/http://www.sapo.pt/noticias/controlado-incendio-em-torre-residencial-no_598431e25e28b30728870262: 404 Client Error: Not Found for url: https://arquivo.pt/noFrame/replay/20170815152855/http://24.sapo.pt/noticias/internacional/artigo/controlado-incendio-em-torre-residencial-no-dubai_22757989.html\n"
     ]
    }
   ],
   "source": [
    "for index, row in df_artigos.iterrows():\n",
    "    url = row['link']\n",
    "    cleaned_content = scrape_and_clean_article(url)\n",
    "    if cleaned_content:\n",
    "        df_artigos.at[index, 'contents'] = cleaned_content  # Preencher a coluna 'contents' com o conteúdo limpo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Guardar os dados em um CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>termo</th>\n",
       "      <th>site</th>\n",
       "      <th>ano</th>\n",
       "      <th>titulo</th>\n",
       "      <th>data</th>\n",
       "      <th>link</th>\n",
       "      <th>contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>incêndio</td>\n",
       "      <td>www.publico.pt</td>\n",
       "      <td>2017</td>\n",
       "      <td>Incêndio em Gaia - PÚBLICO</td>\n",
       "      <td>2017-01-18 04:36:37</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201701180436...</td>\n",
       "      <td>(cookies cookies porpaulo pimenta 25 10 2016 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>incêndio</td>\n",
       "      <td>www.publico.pt</td>\n",
       "      <td>2017</td>\n",
       "      <td>Complexo turístico Zmar recupera as infra-estr...</td>\n",
       "      <td>2017-01-10 20:21:26</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201701102021...</td>\n",
       "      <td>(obras recuperacao edificios infra estruturas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>incêndio</td>\n",
       "      <td>www.publico.pt</td>\n",
       "      <td>2017</td>\n",
       "      <td>Zambujeira do Mar - PÚBLICO</td>\n",
       "      <td>2017-01-11 02:26:52</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201701110226...</td>\n",
       "      <td>(cookies cookies todos porcarlos dias 10 01 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>incêndio</td>\n",
       "      <td>www.publico.pt</td>\n",
       "      <td>2017</td>\n",
       "      <td>Ferreira do Zêzere - PÚBLICO</td>\n",
       "      <td>2017-01-11 06:05:20</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201701110605...</td>\n",
       "      <td>(cookies esti utilizazo cookies localizazo tod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>incêndio</td>\n",
       "      <td>www.publico.pt</td>\n",
       "      <td>2017</td>\n",
       "      <td>Andrzej Zulawski (1940 - 2016) - PÚBLICO</td>\n",
       "      <td>2017-06-22 05:00:38</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201706220500...</td>\n",
       "      <td>(cookies cookies todos porluis miguel queirose...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>seca</td>\n",
       "      <td>www.cmjornal.pt</td>\n",
       "      <td>2018</td>\n",
       "      <td>Vocalista dos Xutos surpreendido com homenagem...</td>\n",
       "      <td>2018-02-06 08:11:35</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201802060811...</td>\n",
       "      <td>(tim vocalista banda portuguesa xutos pontapes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>seca</td>\n",
       "      <td>www.cmjornal.pt</td>\n",
       "      <td>2018</td>\n",
       "      <td>Coro canta ‘Xutos &amp; Pontapés’ e recorda Zé Ped...</td>\n",
       "      <td>2018-02-02 22:52:34</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201802022252...</td>\n",
       "      <td>(nao unico olhar ceu cantam xutos pontapes dom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>seca</td>\n",
       "      <td>www.cmjornal.pt</td>\n",
       "      <td>2018</td>\n",
       "      <td>Apple acaba com iPhone X já este verão - Tecno...</td>\n",
       "      <td>2018-02-07 03:30:17</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201802070330...</td>\n",
       "      <td>(pensar comprar iphone x entao pense melhor ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>seca</td>\n",
       "      <td>www.cmjornal.pt</td>\n",
       "      <td>2018</td>\n",
       "      <td>Shawn Mendes atua no festival Sudoeste na Zamb...</td>\n",
       "      <td>2018-02-06 23:30:27</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201802062330...</td>\n",
       "      <td>(pub pub pub pub copyright 2018 todos direitos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>seca</td>\n",
       "      <td>www.sapo.pt</td>\n",
       "      <td>2018</td>\n",
       "      <td>Portugal tem a maior almofada financeira da Un...</td>\n",
       "      <td>2018-07-20 14:27:44</td>\n",
       "      <td>https://arquivo.pt/noFrame/replay/201807201427...</td>\n",
       "      <td>(portugal terminou primeiro trimestre ano terc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>527 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        termo             site   ano  \\\n",
       "0    incêndio   www.publico.pt  2017   \n",
       "1    incêndio   www.publico.pt  2017   \n",
       "2    incêndio   www.publico.pt  2017   \n",
       "3    incêndio   www.publico.pt  2017   \n",
       "4    incêndio   www.publico.pt  2017   \n",
       "..        ...              ...   ...   \n",
       "522      seca  www.cmjornal.pt  2018   \n",
       "523      seca  www.cmjornal.pt  2018   \n",
       "524      seca  www.cmjornal.pt  2018   \n",
       "525      seca  www.cmjornal.pt  2018   \n",
       "526      seca      www.sapo.pt  2018   \n",
       "\n",
       "                                                titulo                 data  \\\n",
       "0                           Incêndio em Gaia - PÚBLICO  2017-01-18 04:36:37   \n",
       "1    Complexo turístico Zmar recupera as infra-estr...  2017-01-10 20:21:26   \n",
       "2                          Zambujeira do Mar - PÚBLICO  2017-01-11 02:26:52   \n",
       "3                         Ferreira do Zêzere - PÚBLICO  2017-01-11 06:05:20   \n",
       "4             Andrzej Zulawski (1940 - 2016) - PÚBLICO  2017-06-22 05:00:38   \n",
       "..                                                 ...                  ...   \n",
       "522  Vocalista dos Xutos surpreendido com homenagem...  2018-02-06 08:11:35   \n",
       "523  Coro canta ‘Xutos & Pontapés’ e recorda Zé Ped...  2018-02-02 22:52:34   \n",
       "524  Apple acaba com iPhone X já este verão - Tecno...  2018-02-07 03:30:17   \n",
       "525  Shawn Mendes atua no festival Sudoeste na Zamb...  2018-02-06 23:30:27   \n",
       "526  Portugal tem a maior almofada financeira da Un...  2018-07-20 14:27:44   \n",
       "\n",
       "                                                  link  \\\n",
       "0    https://arquivo.pt/noFrame/replay/201701180436...   \n",
       "1    https://arquivo.pt/noFrame/replay/201701102021...   \n",
       "2    https://arquivo.pt/noFrame/replay/201701110226...   \n",
       "3    https://arquivo.pt/noFrame/replay/201701110605...   \n",
       "4    https://arquivo.pt/noFrame/replay/201706220500...   \n",
       "..                                                 ...   \n",
       "522  https://arquivo.pt/noFrame/replay/201802060811...   \n",
       "523  https://arquivo.pt/noFrame/replay/201802022252...   \n",
       "524  https://arquivo.pt/noFrame/replay/201802070330...   \n",
       "525  https://arquivo.pt/noFrame/replay/201802062330...   \n",
       "526  https://arquivo.pt/noFrame/replay/201807201427...   \n",
       "\n",
       "                                              contents  \n",
       "0    (cookies cookies porpaulo pimenta 25 10 2016 1...  \n",
       "1    (obras recuperacao edificios infra estruturas ...  \n",
       "2    (cookies cookies todos porcarlos dias 10 01 20...  \n",
       "3    (cookies esti utilizazo cookies localizazo tod...  \n",
       "4    (cookies cookies todos porluis miguel queirose...  \n",
       "..                                                 ...  \n",
       "522  (tim vocalista banda portuguesa xutos pontapes...  \n",
       "523  (nao unico olhar ceu cantam xutos pontapes dom...  \n",
       "524  (pensar comprar iphone x entao pense melhor ap...  \n",
       "525  (pub pub pub pub copyright 2018 todos direitos...  \n",
       "526  (portugal terminou primeiro trimestre ano terc...  \n",
       "\n",
       "[527 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_artigos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web scraping e limpeza concluídos! Dados guardados em 'detalhes_artigos_completo.csv'.\n"
     ]
    }
   ],
   "source": [
    "df_artigos.to_csv('detalhes_artigos_completo.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"Web scraping e limpeza concluídos! Dados guardados em 'detalhes_artigos_completo.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ED_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
